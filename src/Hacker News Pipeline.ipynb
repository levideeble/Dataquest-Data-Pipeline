{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Throughout the last module in the Dataquest course, we learned the concepts of functional programming and then built our own data pipeline class in Python. We learned about advanced Python concepts such as decorators, closures and good API design. We also learned how to implement a Directed Acyclic Graph (DAG) as the scheduler for our pipeline.\n",
    "\n",
    "After completing all of these lessons, we finally built a robust data pipeline that schedules our tasks in the correct order. In this project, we will use the pipeline on a real world data pipeline project.\n",
    "From a JSON API, we will filter, clean, aggregate, and summarise data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "The data we will use comes from a [Hacker News](https://news.ycombinator.com/) (HN) API that returns JSON data of the top stories in 2014. Hacker News is a link aggregator website where users vote up stories that are interesting to the computer science, science and entrepreneur communities.\n",
    "\n",
    "We will use a pre-downloaded JSON file called `hn_stories_2014.json`, that contains a single key, `stories` - a list of stories (posts). Each post has a set of keys, but we will only deal with the following keys:\n",
    "\n",
    "* `created_at`: A timestamp of the story's creation time.\n",
    "* `created_at_i`: A unix epoch timestamp.\n",
    "* `url`: The URL of the story link.\n",
    "* `objectID`: The ID of the story.\n",
    "* `author`: The story's author (username on HN).\n",
    "* `points`: The number of upvotes the story had. \n",
    "* `title`: The headline of the post.\n",
    "* `num_comments`: The number of comments a post has.\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. The goal is to find the top 100 keywords of HN posts in 2014. This will give us an understanding of the most talked about tech topics at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pipeline class that we created in the previous lesson and instantiate an instance of it.\n",
    "from pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data\n",
    "\n",
    "Let's start by loading the JSON file data into Python. As JSON files resemble a key-value dictionary, we will parse it into a Python `dict` object using the `json` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task to load in the JSON data\n",
    "import json\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "        return stories # Returns a list of dict objects, with each one being an individual story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Stories\n",
    "\n",
    "Now we have loaded in all the stories as a list of `dict` objects, we can now operate on them. Let's start by filtering the list of stories to get the most popular stories of the year.\n",
    "\n",
    "We can filter for popular stories by ensuring they are links (not `Ask HN` posts), have a good number of points, and have some comments. We will consider a good number of points to be above 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline\n",
    "import json\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "        return stories\n",
    "\n",
    "# Create a task to filter the stories for popular ones.\n",
    "@pipeline.task(depends_on=file_to_json) # Set the dependency to ensure the task runs after file_to_json and uses its results\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['title'].split(' ')[0:2] != 'Ask HN' and story['points'] > 50 and story['num_comments'] > 1\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV\n",
    "\n",
    "With a reduced set of stories, it's time to write these `dict` objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summaries. By keeping consistent data formats, each pipeline task will be adaptable with future task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, build_csv\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "        return stories\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['title'].split(' ')[0:2] != 'Ask HN' and story['points'] > 50 and story['num_comments'] > 1\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "\n",
    "# Create a task the writes the filtered JSON stories to a CSV file.\n",
    "@pipeline.task(depends_on=filter_stories) # Set the dependency to filter_stories task\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        # parse the created_at column as a datetime object\n",
    "        lines.append((story['objectID'], datetime.strptime(story['created_at'], '%Y-%m-%dT%H:%M:%SZ'), story['url'], story['points'], story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Column\n",
    "\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, build_csv\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "        return stories\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['title'].split(' ')[0:2] != 'Ask HN' and story['points'] > 50 and story['num_comments'] > 1\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append((story['objectID'], datetime.strptime(story['created_at'], '%Y-%m-%dT%H:%M:%SZ'), story['url'], story['points'], story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
    "\n",
    "# Create a task that extracts the title from each story.\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    index = header.index('title')\n",
    "    return (line[index] for line in reader) # Return a generator of each title    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Titles\n",
    "\n",
    "To create our word frequency model from Hacker News titles, we need to ensure that we use a consistent set of words. This means that we need to clean the titles by removing punctuation and standardising the strings to lower case.\n",
    "This will prevent words like `Google`, `google` and `google?` from being classed as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, build_csv\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "        return stories\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['title'].split(' ')[0:2] != 'Ask HN' and story['points'] > 50 and story['num_comments'] > 1\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append((story['objectID'], datetime.strptime(story['created_at'], '%Y-%m-%dT%H:%M:%SZ'), story['url'], story['points'], story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    index = header.index('title')\n",
    "    return (line[index] for line in reader)\n",
    "\n",
    "# Create a task to clean the titles\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    punctuation = string.punctuation\n",
    "    for title in titles:\n",
    "        for character in punctuation:\n",
    "            if character in title:\n",
    "                title = title.replace(character, '') # Remove punctuation\n",
    "        \n",
    "        yield lower(title) # Cast title as lower case\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Word Frequency Dictionary\n",
    "\n",
    "We will now build a word frequency dictionary that consists of key-value pairs of each word that features in the HN titles, along with how many times they occur.\n",
    "To ensure that we find actual keywords, we will enforce the dictionary so that it doesn't include **stop words**. Stop words are those commonly used in language, such as `the`, `i` and `are` ect. These are commonly rejected in keyword searches.\n",
    "We will use the `stop_words` module included in the directory, which contains a tuple of such words, to filter our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, build_csv\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "        return stories\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['title'].split(' ')[0:2] != 'Ask HN' and story['points'] > 50 and story['num_comments'] > 1\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append((story['objectID'], datetime.strptime(story['created_at'], '%Y-%m-%dT%H:%M:%SZ'), story['url'], story['points'], story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    index = header.index('title')\n",
    "    return (line[index] for line in reader)\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    punctuation = string.punctuation\n",
    "    for title in titles:\n",
    "        for character in punctuation:\n",
    "            if character in title:\n",
    "                title = title.replace(character, '')\n",
    "        \n",
    "        yield title.lower()\n",
    "        \n",
    "# Create a task that generates a word frequency dictionary\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(titles):\n",
    "    frequency_dict = {}\n",
    "    for title in titles:\n",
    "        words = title.split(' ')\n",
    "        for word in words:\n",
    "            if word and word not in stop_words:\n",
    "                if word in frequency_dict:\n",
    "                    frequency_dict[word] += 1\n",
    "                else:\n",
    "                    frequency_dict[word] = 1\n",
    "    \n",
    "    return frequency_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sorting the Top Words\n",
    "\n",
    "Finally, we can sort the words so that we can see the most common keywords used in Hacker News titles. We will output a list of tuples with `(word, frequency)` as entries sorted from most used to least used. We will limit our results to the top 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline, build_csv\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "        return stories\n",
    "\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['title'].split(' ')[0:2] != 'Ask HN' and story['points'] > 50 and story['num_comments'] > 1\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append((story['objectID'], datetime.strptime(story['created_at'], '%Y-%m-%dT%H:%M:%SZ'), story['url'], story['points'], story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    index = header.index('title')\n",
    "    return (line[index] for line in reader)\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    punctuation = string.punctuation\n",
    "    for title in titles:\n",
    "        for character in punctuation:\n",
    "            if character in title:\n",
    "                title = title.replace(character, '')\n",
    "        \n",
    "        yield title.lower()\n",
    "        \n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(titles):\n",
    "    frequency_dict = {}\n",
    "    for title in titles:\n",
    "        words = title.split(' ')\n",
    "        for word in words:\n",
    "            if word and word not in stop_words:\n",
    "                if word in frequency_dict:\n",
    "                    frequency_dict[word] += 1\n",
    "                else:\n",
    "                    frequency_dict[word] = 1\n",
    "    \n",
    "    return frequency_dict\n",
    "\n",
    "# Create a task to sort the word frequencies\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_100(word_frequencies):\n",
    "    sorted_words = sorted(word_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_words[:100]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can see that our pipeline and tasks work as intended, and it would be easy to update or add in tasks as our needs develop. \n",
    "\n",
    "If we look at the results, we find that `Google`, `bitcoin`, `programming` and `web` are popular keywords in Hacker News post titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 185), ('google', 167), ('bitcoin', 102), ('open', 95), ('programming', 92), ('web', 89), ('data', 86), ('video', 79), ('python', 75), ('code', 74), ('facebook', 71), ('released', 71), ('using', 70), ('source', 68), ('2013', 65), ('2014', 65), ('free', 65), ('javascript', 65), ('game', 64), ('internet', 62), ('c', 60), ('work', 59), ('microsoft', 59), ('linux', 58), ('app', 57), ('pdf', 55), ('language', 54), ('software', 54), ('use', 53), ('startup', 52), ('make', 51), ('apple', 50), ('time', 49), ('yc', 48), ('security', 48), ('nsa', 45), ('github', 45), ('windows', 44), ('like', 44), ('project', 42), ('way', 42), ('world', 41), ('users', 40), ('developer', 40), ('1', 40), ('computer', 40), ('heartbleed', 40), ('dont', 38), ('git', 37), ('design', 37), ('ios', 37), ('os', 36), ('twitter', 36), ('ceo', 36), ('online', 36), ('vs', 36), ('big', 36), ('life', 36), ('day', 35), ('android', 34), ('years', 34), ('apps', 34), ('best', 34), ('simple', 33), ('mt', 33), ('court', 33), ('firefox', 32), ('guide', 32), ('learning', 32), ('gox', 32), ('site', 32), ('api', 32), ('says', 32), ('browser', 32), ('server', 31), ('fast', 31), ('problem', 31), ('mozilla', 31), ('engine', 31), ('introducing', 30), ('does', 30), ('amazon', 30), ('better', 30), ('year', 30), ('text', 30), ('support', 29), ('stop', 29), ('tech', 29), ('built', 29), ('million', 29), ('money', 29), ('people', 29), ('3', 28), ('developers', 28), ('development', 28), ('did', 28), ('help', 28), ('learn', 28), ('billion', 27), ('hacker', 27)]\n"
     ]
    }
   ],
   "source": [
    "completed_tasks = pipeline.run()\n",
    "print(completed_tasks[top_100])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
